{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "KyZTroXCzEop",
        "mOX2kBs-zHhv",
        "ECcK_kPTv2sb",
        "jlEcJI8FJ73G",
        "b7jMdwD0pey_",
        "7f30KeVBsrwU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acse-dl1122/git_action_practice/blob/master/Copy_of_DL_module_Coursework_I_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1dFgNX9iQUfmBOdmUN2-H8rPxL3SLXmxn\" width=\"400\"/>\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9YehS8enAmDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Name***: Dingo Luo\n",
        "### ***CID***: 01765358"
      ],
      "metadata": {
        "id": "gD4VUfPGx8Oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions:\n",
        "\n",
        "Follow the instructions below to complete the coursework and submit it:\n",
        "\n",
        "<br>\n",
        "\n",
        "1. Complete your coursework using this provided Jupyter Notebook template (use Google Colab or your local machine if it has a GPU and/or sufficient computational power). Your copy of the notebook should be named: `yourusername_coursework_I.ipynb`. And don't forget to fill in the two fields at the top of this notebook with your name and CID.\n",
        "\n",
        "<br>\n",
        "\n",
        "2. Once you have completed your answers, upload your final notebook to the repo you got from the github classroom link. Make sure to have all the answers in there:\n",
        "\n",
        "   - **All the cells in your final Jupyter Notebook should be executed before saving and uploading to github in order to have the output of the cells available in the uploaded version** (images you plot, training graphs generated with `livelossplot`, etc). We will not rerun code blocks in the notebooks, it is your responsibility to run them before uploading the notebook.\n",
        "\n",
        "   - Add comments in the code to explain what you are doing at every step. \n",
        "\n",
        "   - All answers requiring written answers (ie, not code) should be in markdown blocks in the Jupyter Notebook. This provided Jupyter Notebook template has allocated blocks for the questions, but **you can add any coding or markdown blocks you need**.\n",
        "\n",
        "<br>\n",
        "\n",
        "3. The coursework is released on **Friday 9 December at 14:00h UK time**, and the answers have to be submitted on **Monday 12 December, 20:00h UK time**. We will not accept late submissions.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "### The coursework consists of 5 questions you have to complete. You will find them below. \n",
        "\n",
        "- Questions 2 and 3 are very similar and you have to complete both of them, **you DO NOT have to choose one of them, you have to do them both.**\n",
        "\n",
        "- You can use code seen in class, but indicate clearly when you do, and **make it clear (using comments or markdown blocks) what are your modifications**. This will influence your final marks.\n",
        "\n",
        "- Your final marks will depend both on the quality of your results **AND** the justification, explanation and rationale of your implementations.\n",
        "\n",
        "- Reference any publications and other materials that you use, but keep in mind that **your implementations should be original and not copied from any online resources (we will check for plagiarism)**. It is ok to use other resources to understand concepts and draw inspiration, but always reference them properly in the notebook.\n",
        "\n",
        "- Finally, **justify your answers well, but focus on what the question is asking**. I will penalise lengthy answers that do not have any relation with the question being addressed.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "uRlgsfRcx9sT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "KyZTroXCzEop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install livelossplot"
      ],
      "metadata": {
        "id": "4tM5hg9lQnxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "import torch.autograd as autograd\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from livelossplot import PlotLosses\n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "J7r0q-LczFP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "yT8n0p4T4030"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q1** - Download and prepare the Fashion MNIST dataset [10 points]\n",
        "\n",
        "Get the `FashionMNIST` dataset from `torchvision.datasets`\n",
        "\n",
        "- Inspect the dataset and format the data as you see fit to use it in your next questions. Explain the process you follow.\n",
        "\n",
        "- Plot a matrix of images with 10 examples of each class.\n",
        "\n",
        "- Is the dataset well balanced (does it have similar number of samples for each class)? Generate and present data to support your answer in any form you see fit."
      ],
      "metadata": {
        "id": "mOX2kBs-zHhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Inspect the dataset and format the data as you see fit to use it in your next questions. Explain the process you follow."
      ],
      "metadata": {
        "id": "bBvlEm-JY9fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the trainning data\n",
        "train_data = FashionMNIST(\n",
        "    root = 'data',\n",
        "    train = True, \n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "# Download the testing data\n",
        "test_data = FashionMNIST(\n",
        "    root = 'data',\n",
        "    train = False, \n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "train_data.data.shape, test_data.data.shape"
      ],
      "metadata": {
        "id": "YOJVXNOtMXof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataloade for both dataset\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(dataset = train_data, batch_size = batch_size)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "iFA3a7X-X78g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The data have been splited into target (X) and label (y), which is to be used for the conditional VAE and conditional GAN. \n",
        "\n",
        "X_train = train_data.train_data.view(-1, 1, 28, 28).float()\n",
        "Y_train = train_data.train_labels\n",
        "X_test = test_data.test_data.view(-1, 1, 28, 28).float()\n",
        "Y_test = test_data.test_labels;"
      ],
      "metadata": {
        "id": "CLWh-C-5stik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Plot a matrix of images with 10 examples of each class."
      ],
      "metadata": {
        "id": "qMJqVAqNZJiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting 10 images from each class, and store their index into an dictionary\n",
        "sample = {\n",
        "    0: [],\n",
        "    1: [],\n",
        "    2: [],\n",
        "    3: [],\n",
        "    4: [],\n",
        "    5: [],\n",
        "    6: [],\n",
        "    7: [],\n",
        "    8: [],\n",
        "    9: [],\n",
        "}\n",
        "\n",
        "size = 10\n",
        "\n",
        "check = 0\n",
        "for batch, (x, y) in enumerate(train_data):\n",
        "    if len(sample[y]) < size:\n",
        "        sample[y].append(batch)\n",
        "        check += 1\n",
        "    if check == size * 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "J5BWnu07xF1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yjC_HPZIH3Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_sample_idx = []\n",
        "\n",
        "for y in sample:\n",
        "    for idx in sample[y]:\n",
        "        all_sample_idx.append(idx)"
      ],
      "metadata": {
        "id": "RKKUY50h4CbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = list(train_data.class_to_idx)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "for i in range(10 * 10):\n",
        "    plt.subplot(10, 10, i + 1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(X_train[all_sample_idx[i]].reshape(28, 28))\n",
        "    plt.title('(%s) %s'%(\n",
        "        int(Y_train[all_sample_idx[i]]),\n",
        "        class_to_idx[Y_train[all_sample_idx[i]]]\n",
        "    ))\n",
        "\n",
        "plt.tight_layout(pad=1.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i5DoNpYqWtBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Is the dataset well balanced (does it have similar number of samples for each class)? Generate and present data to support your answer in any form you see fit."
      ],
      "metadata": {
        "id": "mO5RqOqQZLAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the data\n",
        "print(np.unique(train_data.targets, return_counts = True))\n",
        "print(np.unique(test_data.targets, return_counts = True))"
      ],
      "metadata": {
        "id": "x3uOAV3SVEw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the output, for training set and test set both include 10 classes, each class contains 6000 data for training and 1000 for testing. Threfore the data is **well balanced**. "
      ],
      "metadata": {
        "id": "KAoI-_4lVHSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "NxE9bvBV412w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q2** - Implement a **conditional VAE** [25 points]\n",
        "\n",
        "Implement a VAE similar to the one we saw in class but with the following modifications:\n",
        "\n",
        "- Your new network should generate samples of any class defined by the user, that is, when you run your decoder, you have to tell it which class you want it to generate. <br> *During the lectures we created a VAE that would generate samples of any class, we did not have control over which class would be generated. The exercise is to modify the network, training loops, and anything else you consider necessary, so that, once is trained, you can generate images of any particular class specified when you run your generative model.*\n",
        "\n",
        "- Expand your network to include **at least two** of the following modifications. Choose them based on what you think will result in better network performance (make sure to justify your choices well, argumenting why did you decide to include your particular modifications):\n",
        "  - Convolutional layers\n",
        "  - Data augmentation \n",
        "  - Different activations\n",
        "  - Different random seed\n",
        "  - Other network modifications that are well justified. <br> *You have freedom to choose what to do here, but your marks will depend not only on the correct implementation but also on the justification of your choices.*\n",
        "\n",
        "- Use your trained conditional VAE to **generate and plot 10 new samples of each class** by adding the class label to your random input (in any form you have implemented it).\n",
        "\n",
        "- You **DO NOT** have to optimise any hyperparameters, but if you had to choose three hyperparameters to optimise:\n",
        "  - Which ones would you choose and why?\n",
        "\n",
        "Generate information of your training process (liveloss plots, or any other data that will help you understand your results and analyse them in the last question **Q4**).\n",
        "\n"
      ],
      "metadata": {
        "id": "9gDJwiiz0BXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Your new network should generate samples of any class defined by the user, that is, when you run your decoder, you have to tell it which class you want it to generate. "
      ],
      "metadata": {
        "id": "ECcK_kPTv2sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE_Encoder(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Class contains the Encoder (image -> latent).\n",
        "        '''\n",
        "        super(cVAE_Encoder, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(794, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layerMu = nn.Sequential(\n",
        "            nn.Linear(128, 16)\n",
        "        )\n",
        "\n",
        "        self.layerSigma = nn.Sequential(\n",
        "            nn.Linear(128, 16)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, label):  # Custom pytorch modules should follow this structure \n",
        "        '''\n",
        "        x: [float] the MNIST image\n",
        "        label: [Tensor] label of the image after one-hot encode\n",
        "        '''\n",
        "        x = torch.flatten(x, start_dim = 1)  # Reshape the input into a vector (nD to 1D) \n",
        "\n",
        "        # append the one-hot label at the end of the vector. dim 784 -> 794\n",
        "        x = torch.cat([x.cuda(), label.cuda()], 1)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        mu =  self.layerMu(x)\n",
        "        sigma = torch.exp(self.layerSigma(x))\n",
        "        return mu, sigma"
      ],
      "metadata": {
        "id": "PsZkvU0XnXvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE_Decoder(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Class contains the Decoder (latent -> image).\n",
        "        '''\n",
        "\n",
        "        super(cVAE_Decoder, self).__init__()\n",
        "\n",
        "        self.layerLatent = nn.Sequential(\n",
        "            nn.Linear(16, 128)\n",
        "        )\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(512, 784),\n",
        "            nn.Sigmoid() \n",
        "        )\n",
        "\n",
        "    def forward(self, z, label):  # Custom pytorch modules should follow this structure \n",
        "        torch.cat([z.cuda(), label.cuda()], 1)\n",
        "\n",
        "        z = self.layerLatent(z)\n",
        "        z = self.layer1(z)\n",
        "        z = self.layer2(z)\n",
        "        z = self.layer3(z)\n",
        "\n",
        "        # z = z[...,:-10]\n",
        "        return z.reshape((-1,1,28,28))  # Reshape the vector into an image\n",
        "\n",
        "print('done')\n"
      ],
      "metadata": {
        "id": "rvUn7V_4osu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        '''\n",
        "        Class combines the Encoder and the Decoder with a VAE latent space.\n",
        "        '''\n",
        "        super(cVAE, self).__init__()\n",
        "        self.device = device\n",
        "        self.encoder = cVAE_Encoder()\n",
        "        self.decoder = cVAE_Decoder()\n",
        "        self.distribution = torch.distributions.Normal(0, 1)  # Sample from N(0,1)\n",
        "\n",
        "    def sample_latent_space(self, mu, sigma):\n",
        "        z = mu + sigma * self.distribution.sample(mu.shape).to(self.device)  # (1) Sample the latent distribution\n",
        "        kl_div = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()  # A term, which is required for regularisation\n",
        "        return z, kl_div\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        '''\n",
        "        x - [float] A batch of images from the data-loader\n",
        "        '''\n",
        "\n",
        "        mu, sigma = self.encoder(x, label)  ## (1) Generate the latent vectors Mu and Sigma\n",
        "        z, kl_div = self.sample_latent_space(mu, sigma)  ## (2) Generate the latent vector sample \n",
        "        z = self.decoder(z, label)  ## (3) Generate the reconstructed image\n",
        "        return z, kl_div\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "cEbfP4MVpAvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(test_loss,autoencoder, dataloader):\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images.view(images.size(0), 28*28)\n",
        "    labels = F.one_hot(labels, num_classes = 10)\n",
        "    loss = nn.MSELoss()\n",
        "    loss_sum = []\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        recon, _ = autoencoder(image.unsqueeze(0).cuda(), labels[idx].unsqueeze(0).cuda()) \n",
        "        origin = torch.reshape(images[idx], (28, 28))\n",
        "        pred = recon.cpu().detach().squeeze()\n",
        "        loss_sum.append(loss(origin, pred))\n",
        "    test_loss.append((sum(loss_sum)/len(loss_sum)).item())\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "MrmBizTQAPlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(autoencoder, train_data, test_data, kl_div_on=True, epochs=10, device='cuda'):\n",
        "    test_loss = []\n",
        "    train_loss = []\n",
        "    opt = torch.optim.Adam(autoencoder.parameters())\n",
        "    for epoch in range(epochs):  # Run data over numerous epochs\n",
        "        epoch_loss = 0\n",
        "        for batch, label in tqdm(train_data):  # Iterate over the batches of images and labels\n",
        "            batch = batch.to(device)  # Send batch of images to the GPU\n",
        "            opt.zero_grad()  # Set optimiser grad to 0\n",
        "            label = F.one_hot(label, num_classes = 10)\n",
        "            x_hat, KL = autoencoder(batch, label)  # Generate predicted images (x_hat) by running batch of images through autoencoder\n",
        "            loss = ((batch - x_hat)**2).sum() + KL  # Calculate combined loss\n",
        "            epoch_loss += loss.cpu().clone().detach().numpy()\n",
        "            loss.backward()  # Back-propagate\n",
        "            opt.step()  # Step the optimiser\n",
        "        evaluate(test_loss, autoencoder, test_data)\n",
        "        train_loss.append(epoch_loss)\n",
        "\n",
        "\n",
        "        # plt.figure(figsize=(15, 9))\n",
        "        # plt.plot(train_loss, label=\"train_loss\")\n",
        "        # plt.legend(loc='best')\n",
        "        # plt.xlabel(\"epochs\")\n",
        "        # plt.ylabel(\"loss\")\n",
        "        # plt.title(\"train_loss vs. epochs\")\n",
        "        # plt.show()\n",
        "\n",
        "    return autoencoder, test_loss, train_loss  # Return the trained autoencoder (for later analysis)\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "dims_latent = 2  # Maybe increase this an try the t-sne algorithm for visualisation?!\n",
        "model_cVAE = cVAE(device).to(device)\n",
        "history_cVAE, test_loss_cVAE, train_loss_cVAE = train(model_cVAE, train_loader, test_loader, epochs=20, device=device)\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "BijSr1ZspXWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(test_loss_cVAE, label=\"test_loss\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cVAE test loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nVgf3ht0D-t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(train_loss_cVAE, label=\"train_loss\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cVAE train loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GYnR4Xw-QCOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(test_loader))\n",
        "_, ax = plt.subplots(2, 5, figsize=[18.5, 6])\n",
        "\n",
        "images = images.view(images.size(0), 28*28)\n",
        "labels = F.one_hot(labels, num_classes = 10)\n",
        "\n",
        "\n",
        "for n, idx in enumerate(torch.randint(0,images.shape[0], (5,))):\n",
        "    recon, _ = history_cVAE(images[idx].unsqueeze(0).cuda(), labels[idx].unsqueeze(0).cuda()) \n",
        "    ax[0, n].imshow(torch.reshape(images[idx], (28, 28)).squeeze())\n",
        "    ax[1, n].imshow(recon.cpu().detach().squeeze())"
      ],
      "metadata": {
        "id": "3k_Fcf9an1n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Expand your network to include at least two of the following modifications.\n",
        "  - Convolutional layers\n",
        "  - Data augmentation \n",
        "  - Different activations\n",
        "  - Different random seed\n",
        "  - Other network modifications that are well justified."
      ],
      "metadata": {
        "id": "F_Xn4ITsv_mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different activations"
      ],
      "metadata": {
        "id": "jlEcJI8FJ73G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first implimentation that I would like to do is to change different activation functions. There are three common activation function is to be used in cVAE model:\n",
        "- ReLU\n",
        "- Sigmoid\n",
        "- Tanh <br>\n",
        "\n",
        "For the cVAE model above I have used ReLU activation function for each layer except the output layer, this is because the docoder network typically uses a non-linear activation function. When a model with all linear activation function, the output is also linear pattern so that does not have enough complexity.\n",
        "\n",
        "In the code below I will change the ReLU activation function into Sigmoid and Tanh, and compare the result difference between three different activation function. "
      ],
      "metadata": {
        "id": "-GQynEgAoXa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE_Encoder_Sigmoid(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Class contains the Encoder (image -> latent).\n",
        "        '''\n",
        "        super(cVAE_Encoder_Sigmoid, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(794, 512),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.layerMu = nn.Sequential(\n",
        "            nn.Linear(128, 16)\n",
        "        )\n",
        "\n",
        "        self.layerSigma = nn.Sequential(\n",
        "            nn.Linear(128, 16)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, label):  # Custom pytorch modules should follow this structure \n",
        "        '''\n",
        "        x: [float] the MNIST image\n",
        "        label: [Tensor] label of the image after one-hot encode\n",
        "        '''\n",
        "        x = torch.flatten(x, start_dim = 1)  # Reshape the input into a vector (nD to 1D) \n",
        "\n",
        "        # append the one-hot label at the end of the vector. dim 784 -> 794\n",
        "        x = torch.cat([x.cuda(), label.cuda()], 1)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        mu =  self.layerMu(x)\n",
        "        sigma = torch.exp(self.layerSigma(x))\n",
        "        return mu, sigma"
      ],
      "metadata": {
        "id": "mX3-a0rUv_QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE_Decoder_Sigmoid(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Class contains the Decoder (latent -> image).\n",
        "        '''\n",
        "\n",
        "        super(cVAE_Decoder_Sigmoid, self).__init__()\n",
        "\n",
        "        self.layerLatent = nn.Sequential(\n",
        "            nn.Linear(16, 128)\n",
        "        )\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(256, 512),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(512, 784),\n",
        "            nn.Sigmoid() \n",
        "        )\n",
        "\n",
        "    def forward(self, z, label):  # Custom pytorch modules should follow this structure \n",
        "        torch.cat([z.cuda(), label.cuda()], 1)\n",
        "\n",
        "        z = self.layerLatent(z)\n",
        "        z = self.layer1(z)\n",
        "        z = self.layer2(z)\n",
        "        z = self.layer3(z)\n",
        "\n",
        "        #z = z[...,:-10]\n",
        "\n",
        "        return z.reshape((-1,1,28,28))  # Reshape the vector into an image\n",
        "\n",
        "print('done')\n"
      ],
      "metadata": {
        "id": "sD20jpl4p1sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE_Sigmoid(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        '''\n",
        "        Class combines the Encoder and the Decoder with a VAE latent space.\n",
        "        '''\n",
        "        super(cVAE_Sigmoid, self).__init__()\n",
        "        self.device = device\n",
        "        self.encoder = cVAE_Encoder_Sigmoid()\n",
        "        self.decoder = cVAE_Decoder_Sigmoid()\n",
        "        self.distribution = torch.distributions.Normal(0, 1)  # Sample from N(0,1)\n",
        "\n",
        "    def sample_latent_space(self, mu, sigma):\n",
        "        z = mu + sigma * self.distribution.sample(mu.shape).to(self.device)  # (1) Sample the latent distribution\n",
        "        kl_div = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()  # A term, which is required for regularisation\n",
        "        return z, kl_div\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        '''\n",
        "        x - [float] A batch of images from the data-loader\n",
        "        '''\n",
        "\n",
        "        mu, sigma = self.encoder(x, label)  ## (1) Generate the latent vectors Mu and Sigma\n",
        "        z, kl_div = self.sample_latent_space(mu, sigma)  ## (2) Generate the latent vector sample \n",
        "        z = self.decoder(z, label)  ## (3) Generate the reconstructed image\n",
        "        return z, kl_div\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "Y_pVAyVSqIwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(autoencoder, train_data, test_data, kl_div_on=True, epochs=10, device='cuda'):\n",
        "    test_loss = []\n",
        "    train_loss = []\n",
        "    opt = torch.optim.Adam(autoencoder.parameters())\n",
        "    for epoch in range(epochs):  # Run data over numerous epochs\n",
        "        epoch_loss = 0\n",
        "        for batch, label in tqdm(train_data):  # Iterate over the batches of images and labels\n",
        "            batch = batch.to(device)  # Send batch of images to the GPU\n",
        "            opt.zero_grad()  # Set optimiser grad to 0\n",
        "            label = F.one_hot(label, num_classes = 10)\n",
        "            x_hat, KL = autoencoder(batch, label)  # Generate predicted images (x_hat) by running batch of images through autoencoder\n",
        "            loss = ((batch - x_hat)**2).sum() + KL  # Calculate combined loss\n",
        "            epoch_loss += loss.cpu().clone().detach().numpy()\n",
        "            loss.backward()  # Back-propagate\n",
        "            opt.step()  # Step the optimiser\n",
        "        evaluate(test_loss, autoencoder, test_data)\n",
        "        train_loss.append(epoch_loss)\n",
        "\n",
        "    return autoencoder, test_loss, train_loss  # Return the trained autoencoder (for later analysis)\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "dims_latent = 2  # Maybe increase this an try the t-sne algorithm for visualisation?!\n",
        "model_cVAE_Sigmoid = cVAE_Sigmoid(device).to(device)\n",
        "history_cVAE_Sigmoid, test_loss_cVAE_Sigmoid, train_loss_cVAE_Sigmoid = train(model_cVAE_Sigmoid, train_loader, test_loader, epochs=20, device=device)\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "J9gF9tI-qWOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(test_loss_cVAE, label=\"test_loss ReLU\")\n",
        "plt.plot(test_loss_cVAE_Sigmoid, label=\"test_loss Sigmoid\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cVAE test loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5zf5R9VfqzjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(train_loss_cVAE, label=\"train_loss ReLU\")\n",
        "plt.plot(train_loss_cVAE_Sigmoid, label=\"train_loss Sigmoid\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cVAE train loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jSk_FZwIqzda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE_Encoder_Tanh(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Class contains the Encoder (image -> latent).\n",
        "        '''\n",
        "        super(cVAE_Encoder_Tanh, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(794, 512),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layerMu = nn.Sequential(\n",
        "            nn.Linear(128, 16)\n",
        "        )\n",
        "\n",
        "        self.layerSigma = nn.Sequential(\n",
        "            nn.Linear(128, 16)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, label):  # Custom pytorch modules should follow this structure \n",
        "        '''\n",
        "        x: [float] the MNIST image\n",
        "        label: [Tensor] label of the image after one-hot encode\n",
        "        '''\n",
        "        x = torch.flatten(x, start_dim = 1)  # Reshape the input into a vector (nD to 1D) \n",
        "\n",
        "        # append the one-hot label at the end of the vector. dim 784 -> 794\n",
        "        x = torch.cat([x.cuda(), label.cuda()], 1)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        mu =  self.layerMu(x)\n",
        "        sigma = torch.exp(self.layerSigma(x))\n",
        "        return mu, sigma"
      ],
      "metadata": {
        "id": "dArgLeXZryNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE_Decoder_Tanh(nn.Module):  # The Encoder inherits the properties of nn.Module https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Class contains the Decoder (latent -> image).\n",
        "        '''\n",
        "\n",
        "        super(cVAE_Decoder_Tanh, self).__init__()\n",
        "\n",
        "        self.layerLatent = nn.Sequential(\n",
        "            nn.Linear(16, 128)\n",
        "        )\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(256, 512),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(512, 784),\n",
        "            nn.Tanh() \n",
        "        )\n",
        "\n",
        "    def forward(self, z, label):  # Custom pytorch modules should follow this structure \n",
        "        torch.cat([z.cuda(), label.cuda()], 1)\n",
        "\n",
        "        z = self.layerLatent(z)\n",
        "        z = self.layer1(z)\n",
        "        z = self.layer2(z)\n",
        "        z = self.layer3(z)\n",
        "\n",
        "        # z = z[...,:-10]\n",
        "\n",
        "        return z.reshape((-1,1,28,28))  # Reshape the vector into an image\n",
        "\n",
        "print('done')\n"
      ],
      "metadata": {
        "id": "5BYGBC3-r7ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class cVAE_Tanh(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        '''\n",
        "        Class combines the Encoder and the Decoder with a VAE latent space.\n",
        "        '''\n",
        "        super(cVAE_Tanh, self).__init__()\n",
        "        self.device = device\n",
        "        self.encoder = cVAE_Encoder_Tanh()\n",
        "        self.decoder = cVAE_Decoder_Tanh()\n",
        "        self.distribution = torch.distributions.Normal(0, 1)  # Sample from N(0,1)\n",
        "\n",
        "    def sample_latent_space(self, mu, sigma):\n",
        "        z = mu + sigma * self.distribution.sample(mu.shape).to(self.device)  # (1) Sample the latent distribution\n",
        "        kl_div = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()  # A term, which is required for regularisation\n",
        "        return z, kl_div\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        '''\n",
        "        x - [float] A batch of images from the data-loader\n",
        "        '''\n",
        "\n",
        "        mu, sigma = self.encoder(x, label)  ## (1) Generate the latent vectors Mu and Sigma\n",
        "        z, kl_div = self.sample_latent_space(mu, sigma)  ## (2) Generate the latent vector sample \n",
        "        z = self.decoder(z, label)  ## (3) Generate the reconstructed image\n",
        "        return z, kl_div\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "CxRxEO90sGCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model_cVAE_Tanh = cVAE_Tanh(device).to(device)\n",
        "history_cVAE_Tanh, test_loss_cVAE_Tanh, train_loss_cVAE_Tanh = train(model_cVAE_Tanh, train_loader, test_loader, epochs=20, device=device)\n",
        "\n",
        "print('done')"
      ],
      "metadata": {
        "id": "Pb2qp2-isQXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(test_loss_cVAE, label=\"test_loss ReLU\")\n",
        "plt.plot(test_loss_cVAE_Sigmoid, label=\"test_loss Sigmoid\")\n",
        "plt.plot(test_loss_cVAE_Tanh, label=\"test_loss Tanh\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cVAE test loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X0aEOnUXs0aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "plt.plot(train_loss_cVAE, label=\"train_loss ReLU\")\n",
        "plt.plot(train_loss_cVAE_Sigmoid, label=\"train_loss Sigmoid\")\n",
        "plt.plot(train_loss_cVAE_Tanh, label=\"train_loss Tanh\")\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cVAE train loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i6RxI8-Rs7yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the ReLU activation function have better performance in terms of training speed and accuracy. Therefore we can keep the ReLU activation function as the best choice. "
      ],
      "metadata": {
        "id": "VOjA9q79t3D7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different random seeds"
      ],
      "metadata": {
        "id": "VlGXH_bbKBGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In cVAE, the random seed is initialized and to determines the sequence of random numbers that are generated by the model. This can have a significant impace on the performance and accuracy of the model.\n",
        "\n",
        "Implimenting different random seeds in a CVAE allows the model to be trained and evaluated using multiple different sets of random numbers, which is going to provide a more robust estimate of the model's performance. \n",
        "\n",
        "Implimenting differnt random seeds is particularly useful when tring to improve the model's avility to generate new data, as it can help to ensure that the results are not overly influenced by the specific random seed that was used during training.\n",
        "\n",
        "In summary, the use of different random seeds in a cVAE can provide several benefits, including improved generalization, reproducibility, and robustness."
      ],
      "metadata": {
        "id": "ku3fn70ouEC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "owXWUtTVKLqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_train_loss = []\n",
        "seed_test_loss = []\n",
        "seed_number = []\n",
        "for i in range(5):\n",
        "    random_seed_model_name = f'history_cVAE_seed_{i}'\n",
        "\n",
        "    set_seed(i)\n",
        "\n",
        "    model_cVAE_seed = cVAE_Tanh(device).to(device)\n",
        "    random_seed_model_name, test_loss_cVAE, train_loss_cVAE = train(model_cVAE, train_loader, test_loader, epochs=20, device=device)\n",
        "\n",
        "    seed_number.append(i)\n",
        "    seed_train_loss.append(train_loss_cVAE)\n",
        "    seed_test_loss.append(test_loss_cVAE)\n",
        "    print(i)\n",
        "    print('=============================seed done======================')\n"
      ],
      "metadata": {
        "id": "J3-6E5I9vaLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "8mk71Bry6L3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "for i in range(len(seed_number)):\n",
        "    plt.plot(seed_test_loss[i], label=i)\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cVAE test loss with different random seed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vmyLt9_U2c23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 9))\n",
        "for i in range(len(seed_number)):\n",
        "    plt.plot(seed_train_loss[i], label=i)\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.title(\"cVAE train loss with different random seed\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SijCnr_R2-Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from above, different random seed number do result in better loss for the training dataset, but for the testing data set, there have no much impact into the loss. Therefore"
      ],
      "metadata": {
        "id": "rY8mLqalMmYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Use your trained conditional VAE to **generate and plot 10 new samples of each class** by adding the class label to your random input (in any form you have implemented it).\n"
      ],
      "metadata": {
        "id": "b7jMdwD0pey_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.set_grad_enabled(False):\n",
        "    generation_class = torch.arange(10)\n",
        "    generation_class = generation_class.repeat_interleave(10)\n",
        "    gen_images = torch.FloatTensor(np.random.randn(100*28*28).reshape(-1,28,28)).cuda()\n",
        "    gen_labels = F.one_hot(generation_class, num_classes = 10).cuda()\n",
        "    generations, loss = history_cVAE(gen_images, gen_labels)"
      ],
      "metadata": {
        "id": "s-Vvo65BBmHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_gallary(images, labels, rows, cols, title):\n",
        "    class_to_idx = list(train_data.class_to_idx)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i in range(rows * cols):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(torch.reshape(images[i], (28, 28)).squeeze().cpu().clone().numpy())\n",
        "        plt.title('(%s) %s'%(\n",
        "            int(labels[i]),\n",
        "            class_to_idx[labels[i]]\n",
        "        ))\n",
        "    plt.tight_layout(pad=1.1)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    \n",
        "plot_gallary(generations, generation_class, 10, 10, 'Generated images from cVAE using random input and a label')"
      ],
      "metadata": {
        "id": "45w4EkG-A6tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 You **DO NOT** have to optimise any hyperparameters, but if you had to choose three hyperparameters to optimise:\n"
      ],
      "metadata": {
        "id": "c_7WyWyqpouI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first hyperparameter that I would like to optimise is the size & the number of layers in the encoder and decoder. The size and number of layers is used to determine the capacity of the model to learn complex distributions. The correct size and number of layers can improve the model's ability to accurately encode and decode data. A smaller and more efficient cVAE is less computationally complex and can be trained more quickly, which made it more practical for use in real-world applications. "
      ],
      "metadata": {
        "id": "53NLb0LSBVYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second hyperparameter that I want to optimise is the optimization algorithm. By selecting the approporate optimization algorithm, it is possible to train the model more efficiently and effectively, leading to better performance on the dataset. "
      ],
      "metadata": {
        "id": "9vqj857Sdi9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third hyperparameter that I need to consider is the batch sizes. The batch size determines the number of samples that are processed by the model before the weights are updated. With a larger batch size result in accelerate trainning, but less stable gradient and higher generalization error. With smaller batch size can have better generalization but slower training. Overall, finding the optimal values for the batch size can help to balnace trade-offs and improve the performance of the cVAE"
      ],
      "metadata": {
        "id": "fRjQVlJ6BVJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "dCQJ1CIF42n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q3** - Implement a **conditional GAN** that generates a user-defined class [25 points]\n",
        "\n",
        "Implement a GANs similar to the one we saw in class but with the following modifications:\n",
        "\n",
        "- Your new network should generate samples of any class defined by the user, that is, when you run your generator, you have to tell it which class you want it to generate. <br> *During the lectures we created a GAN that would generate samples of any class, we did not have control over which class would be generated. The exercise is to modify the network, training loops, and anything else you consider necessary, so that, once is trained, you can generate images of any particular class specified when you run your generative model.*\n",
        "\n",
        "- Expand your network to include **at least two** of the following modifications. Choose them based on what you think will result in better network performance (make sure to justify your choices well, argumenting why did you decide to include your particular modifications):\n",
        "  - Convolutional layers\n",
        "  - Data augmentation\n",
        "  - Different activations\n",
        "  - Different random seed\n",
        "  - Other network modifications that are well justified. <br> *You have freedom to choose what to do here, but your marks will depend not only on the correct implementation but also on the justification of your choices.*\n",
        "\n",
        "- Use your trained conditional GAN to **generate and plot 10 new samples of each class** by adding the class label to your random input (in any form you have implemented it).\n",
        "\n",
        "- You **DO NOT** have to optimise any hyperparameters, but if you had to choose three hyperparameters to optimise:\n",
        "  - Which ones would you choose and why?\n",
        "  - Would they be different from your choices in **Q2**?\n",
        "\n",
        "\n",
        "Generate information of your training process (liveloss plots, or any other data that will help you understand your results and analyse them in the last question **Q4**)."
      ],
      "metadata": {
        "id": "uXC9kTie3MRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Your new network should generate samples of any class defined by the user, that is, when you run your generator, you have to tell it which class you want it to generate."
      ],
      "metadata": {
        "id": "WNLQhgP-PINw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, g_input_dim=100+10, g_output_dim=28*28):\n",
        "#         super().__init__()       \n",
        "#         self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "#         self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "#         self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "#         self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "    \n",
        "#     # forward method\n",
        "#     def forward(self, x, label): \n",
        "#         x = torch.cat([x.cuda(), label.cuda()], 1)\n",
        "#         x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "#         x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "#         x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "#         return torch.tanh(self.fc4(x))"
      ],
      "metadata": {
        "id": "JZrlUj7U4KRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for testing\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(100 + 10, 256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(256, 512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(512, 1024),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(1024, 784),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def forward(self, z, label):\n",
        "    # Concatenate noise and label\n",
        "    z_label = torch.cat((z.cuda(), label.cuda()), dim=1)\n",
        "    z = self.model(z_label)\n",
        "    return z"
      ],
      "metadata": {
        "id": "xERz11NuKNvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Discriminator(nn.Module):\n",
        "#     def __init__(self, d_input_dim=28*28+10):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "#         self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "#         self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "#         self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "#     # forward method\n",
        "#     def forward(self, x, label):\n",
        "\n",
        "#         x = x.view((bs,-1))\n",
        "#         x = torch.cat([x.cuda(), label.cuda()], 1)\n",
        "\n",
        "#         x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "#         x = F.dropout(x, 0.3)\n",
        "#         x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "#         x = F.dropout(x, 0.3)\n",
        "#         x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "#         x = F.dropout(x, 0.3)\n",
        "#         return self.fc4(x)\n",
        "    "
      ],
      "metadata": {
        "id": "dIJpP73wPMre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for testing\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(784 + 10, 1024),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x, label):\n",
        "    # Concatenate image and label\n",
        "    img_label = torch.cat((x.view(x.size(0), 784).cuda(), label.cuda()), dim=1)\n",
        "    x = self.model(img_label)\n",
        "    return x"
      ],
      "metadata": {
        "id": "uiFBk5TUKUhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)"
      ],
      "metadata": {
        "id": "fToebYcuPVjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameters\n",
        "\n",
        "criterion = nn.BCELoss() \n",
        "z_dim = 100\n",
        "bs = 100\n",
        "lambda_gp = 10\n",
        "n_critic = 5\n",
        "\n",
        "# optimizer\n",
        "lr = 0.0001"
      ],
      "metadata": {
        "id": "yYHeXWq0PbdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset = train_data, batch_size = bs)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size = bs)"
      ],
      "metadata": {
        "id": "ZCz5uh5cR-F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJViqDULLnRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_optimizer = torch.optim.Adam(G.parameters(), lr = lr)\n",
        "D_optimizer = torch.optim.Adam(D.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "LsHxPCF6Pv3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gp_term(x_real, x_fake, label):\n",
        "    m = bs\n",
        "    epsilon = torch.rand(m, 1).to(device)\n",
        "    x_interp = (epsilon * x_real + (1 - epsilon) * x_fake).to(device)\n",
        "    x_interp = Variable(x_interp, requires_grad = True) # so we can compute the grad\n",
        "    D_interp = D(x_interp, label)\n",
        "    \n",
        "    grad_D_interp = autograd.grad(outputs = D_interp,\n",
        "                 inputs = x_interp,\n",
        "                 grad_outputs=torch.ones((bs, 1)).to(device),\n",
        "                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "                 \n",
        "    D_gp = (grad_D_interp.norm(2, dim=1) - 1) ** 2\n",
        "\n",
        "    return D_gp"
      ],
      "metadata": {
        "id": "AqiqoxR9QF9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def D_train(x, lambda_gp, label):\n",
        "    #-------------- Function of the discriminator training -------------------#\n",
        "    D.train()\n",
        "    D_optimizer.zero_grad()\n",
        "\n",
        "    # D_real \n",
        "    x_real = x.view(-1, 28*28)\n",
        "    x_real= Variable(x_real.to(device))\n",
        "    D_real = D(x_real, label)\n",
        "\n",
        "    # D_fake\n",
        "    # sample vector and produce generator output\n",
        "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    x_fake = G(z, label)\n",
        "    D_fake = D(x_fake, label)\n",
        "\n",
        "    # D_gp\n",
        "    D_gp = gp_term(x_real, x_fake, label)\n",
        "    \n",
        "    # combine the losses\n",
        "    D_loss = D_fake.mean() - D_real.mean() + lambda_gp * D_gp.mean()\n",
        "\n",
        "    # model update \n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "        \n",
        "    return  D_loss.data.item()"
      ],
      "metadata": {
        "id": "Wsr94SsrQKIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def G_train(x, label):\n",
        "    #-------------- Function of the generator training -------------------#\n",
        "    G.train()\n",
        "    G_optimizer.zero_grad()\n",
        "\n",
        "    # sample vector and produce generator output\n",
        "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    x_fake = G(z, label)\n",
        "\n",
        "    # D_output for x_fake\n",
        "    D_output = D(x_fake, label)\n",
        "\n",
        "    # G loss\n",
        "    G_loss = - D_output.mean() \n",
        "\n",
        "    # model update \n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "        \n",
        "    return G_loss.data.item()"
      ],
      "metadata": {
        "id": "MrXK09Y4QNcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 100 # 200 epochs is about 40 minutes\n",
        "groups = {'Loss': ['D_Loss', 'G_Loss']}\n",
        "liveloss = PlotLosses(groups=groups)\n",
        "\n",
        "for epoch in range(1, n_epoch+1):  \n",
        "  D_losses, G_losses = [], []\n",
        "  logs = {}\n",
        "  \n",
        "  for batch_idx, (x, y) in enumerate(train_loader):\n",
        "    y = F.one_hot(y, num_classes = 10)\n",
        "    logs['D_Loss'] = D_train(x, lambda_gp, y) \n",
        "    if (batch_idx % n_critic == 0):\n",
        "      logs['G_Loss'] = G_train(x, y)\n",
        "  liveloss.update(logs)\n",
        "  liveloss.draw()\n",
        "\n",
        "  # save every 20th epochs\n",
        "  if(np.mod(epoch, 10) == 0):\n",
        "    torch.save(G.state_dict(), \"./Generator_{:03d}.pth\".format(epoch))\n"
      ],
      "metadata": {
        "id": "p0awZBRVQQJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "def allDone():\n",
        "  display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
        "\n",
        "allDone()"
      ],
      "metadata": {
        "id": "RbqYCV6Lo8-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Expand your network to include **at least two** of the following modifications\n",
        "  - Convolutional layers\n",
        "  - Data augmentation\n",
        "  - Different activations\n",
        "  - Different random seed\n",
        "  - Other network modifications that are well justified.\n"
      ],
      "metadata": {
        "id": "MfQoOjYS5cW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different random seed"
      ],
      "metadata": {
        "id": "7f30KeVBsrwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first modification that I would like to impliment is the random seed. This is because the random noise vector is added to the generator, and the generator use it as a starting point to generate new data samples. When the random noise is too close to a real image(data), the discriminator will have a difficult time determining whether the output is real or fake. Therefore changing the random seed are very important for the cGAN model, "
      ],
      "metadata": {
        "id": "hYHE9DUTq4mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cGAN(n_epoch, seed):\n",
        "    groups = {'Loss': ['D_Loss', 'G_Loss']}\n",
        "    liveloss = PlotLosses(groups=groups)\n",
        "\n",
        "    for epoch in range(1, n_epoch+1):  \n",
        "        D_losses, G_losses = [], []\n",
        "        logs = {}\n",
        "    \n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            y = F.one_hot(y, num_classes = 10)\n",
        "            logs['D_Loss'] = D_train(x, lambda_gp, y) \n",
        "            if (batch_idx % n_critic == 0):\n",
        "                logs['G_Loss'] = G_train(x, y)\n",
        "        liveloss.update(logs)\n",
        "        liveloss.draw()\n",
        "\n",
        "    \n",
        "        # save every 20th epochs\n",
        "        if(np.mod(epoch, 10) == 0):\n",
        "            torch.save(G.state_dict(), \"./Generator_{:03d}_{:03d}.pth\".format(epoch, seed))\n"
      ],
      "metadata": {
        "id": "YeMgg0xNtGPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "    set_seed(i)\n",
        "    train_cGAN(10, i)"
      ],
      "metadata": {
        "id": "oZQeCUgLtpO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G.load_state_dict(torch.load(\"./Generator_{:03d}_{:03d}.pth\".format(10, 1)))"
      ],
      "metadata": {
        "id": "XHtIO29svoDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different activation function"
      ],
      "metadata": {
        "id": "tvf8v7QLxpAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the code above, I have implimented leakyReLU for all the layers as the activation function. I would also like to impliment ReLU function, which introduce the non-linearity into the network and allowing complex model distributions. The Sigmoid function can also be ussed to at the output layer to squash the output values between 0 and 1, making it easier to imterpret the output as a probability. The Tanh function is similar to Sigmoid but squash the output between -1 and 1, which is less useful in our case. "
      ],
      "metadata": {
        "id": "W7f1pDjEzLR5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so4I3RIuCiDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.3 Use your trained conditional GAN to **generate and plot 10 new samples of each class** by adding the class label to your random input (in any form you have implemented it).\n"
      ],
      "metadata": {
        "id": "jy0P8RgI5dCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.utils import save_image\n",
        "set_seed(0)\n",
        "\n",
        "epoch = 100\n",
        "G.load_state_dict(torch.load(\"./Generator_{:03d}.pth\".format(epoch)))\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    generation_class = torch.arange(10)\n",
        "    generation_class = generation_class.repeat_interleave(10)\n",
        "    gen_labels = F.one_hot(generation_class, num_classes = 10).cuda()\n",
        "    test_z = Variable(torch.randn(100, z_dim).to(device))\n",
        "    generated = G(test_z, gen_labels)\n",
        "\n",
        "plot_gallary(generated.view(generated.size(0), 28, 28).cpu(), generation_class, 10, 10, 'Epoch = {:03d}'.format(epoch))\n"
      ],
      "metadata": {
        "id": "KNlOC6PNBGGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "improve the generalization of the model. "
      ],
      "metadata": {
        "id": "HCv7-yFggifw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vuyHi3GLghvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 3.4 You **DO NOT** have to optimise any hyperparameters, but if you had to choose three hyperparameters to optimise:\n",
        "  - Which ones would you choose and why?\n",
        "  - Would they be different from your choices in **Q2**?text block for your answers"
      ],
      "metadata": {
        "id": "jvLxNDBVhHSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first hyperparameter that I want to optimise is the learning rate. As we can see from the loss_plot for the cGAN above, the G_loss are having huge changes for each epochs, and seems to stuck at one place. Optimizing the learning rate can control the speed of updating weight during the trainning and can help improve the performance of the model by ensuring that the parameters are updated in a way that allows the model to learn effeitively from the data. What's more, find the optimal learning rate can help preventr overfitting. <br>\n",
        "This is different from Q2 is because for Q2, the loss of the model decrease smoothly as the epochs increase. "
      ],
      "metadata": {
        "id": "vYX58u84hXPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other two hyperparameters that I want to optimmise is the batch size and the structure of the layers. The reasons are the same for questions 2 with cVAE. "
      ],
      "metadata": {
        "id": "wkX8bj5qnDn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "XsytFvvd43ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q4** - Analysis and comparison of results [20 points]\n",
        "\n",
        "Analyse and compare the results you have obtained with the two networks above.\n",
        "\n",
        "- Use the data you generated training the two networks above to discuss the results you have generated in **Q2** and **Q3**.\n",
        "- Is there a network that performs better than the other? Why do you think that is?\n",
        "- What could you do to improve your results on each of the networks you have implemented? ***\\[limit your answer to this question to 150 words\\]***\n",
        "\n",
        "**IMPORTANT NOTE**: even if you did not complete **Q2** and **Q3**, make sure to include a discussion with any partial results or other considerations regarding the performance of your implemented methods."
      ],
      "metadata": {
        "id": "7vkmbhgx4Zm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here, if you need any (add as many code blocks as you need)"
      ],
      "metadata": {
        "id": "z6EL0Qm1wvfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text block for your answers"
      ],
      "metadata": {
        "id": "oNNJTWp56MRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "R79Gp-Yc45-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q5** - Modify your dataset to create data imbalance and retrain **one** of your conditional models [20 points]\n",
        "\n",
        "Modify the `FashionMNIST` dataset to eliminate 90% of the images corresponding to one of the classes in the dataset (whichever you want).\n",
        "\n",
        "Now, **choose one of the two questions below (a or b)**:\n",
        "\n",
        "- **Q5-a) Retrain your conditional VAE from scratch** with this new dataset, and after training, use it to generate 10 images for the class you have decimated.\n",
        "\n",
        "- **Q5-b) Retrain your conditional GAN from scratch** with this new dataset, and after training, use it to generate 10 images for the class you have decimated.\n",
        "\n",
        "- Compare the results of this new images generated with the modified dataset with the original results from questions **Q2** or **Q3** (depending of whether you have chosen **Q5-a** or **Q5-b**). What do you observe? Describe your interpretation of this comparison."
      ],
      "metadata": {
        "id": "j4VacjK50Rcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here (add as many code blocks as you need)"
      ],
      "metadata": {
        "id": "s6XhR5yT1P77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text block for your answers"
      ],
      "metadata": {
        "id": "Emu4gNjE3SXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here, if you need any (add as many code blocks as you need)"
      ],
      "metadata": {
        "id": "Fd8MFq1E3Uxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text block for your answers"
      ],
      "metadata": {
        "id": "uUA3TW6a3YVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing/"
      ],
      "metadata": {
        "id": "fUlXuztJ3nvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR_G=0.0001\n",
        "LR_D=0.0001\n",
        "Batch_size=100"
      ],
      "metadata": {
        "id": "S3lk68Ci3oym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2YQszsL3sjL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}